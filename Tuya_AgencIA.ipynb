{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9XVrZDNXNtw7ytyMyPhR6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lumpenvisual/Tuya_AgencIA/blob/main/Tuya_AgencIA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üß† TUYA AGENCIA ‚Äì Agencia de IA Aut√≥noma para Creadores de Contenido\n",
        "# üéØ Canal piloto: @turbofausto\n",
        "# üìå Ejecuta paso a paso en Google Colab\n",
        "\n",
        "print(\"üöÄ Bienvenido a Tuya AgencIA ‚Äì Agencia de IA Aut√≥noma\")\n",
        "print(\"Vamos a analizar el canal de @turbofausto usando solo datos p√∫blicos de YouTube.\\n\")"
      ],
      "metadata": {
        "id": "bIdXMltku6y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Instalaci√≥n de dependencias"
      ],
      "metadata": {
        "id": "_VysFfewu_N7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1: Instalar librer√≠as necesarias\n",
        "print(\"üîß Instalando dependencias...\")\n",
        "\n",
        "!pip install -q pytube pandas numpy matplotlib seaborn openai python-dotenv transformers torch gradio\n",
        "\n",
        "!pip install -q scikit-learn          # Para modelos tradicionales (regresi√≥n, clustering)\n",
        "!pip install -q torch torchvision     # PyTorch (deep learning)\n",
        "!pip install -q tensorflow            # TensorFlow (redes neuronales, modelos predictivos)\n",
        "!pip install google-api-python-client\n",
        "!pip install -q transformers          # Hugging Face Transformers\n",
        "!pip install -q datasets              # Hugging Face Datasets\n",
        "\n",
        "print(\"‚úÖ Instalaci√≥n completada.\")"
      ],
      "metadata": {
        "id": "1cE9eaGDvAWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Importar librer√≠as"
      ],
      "metadata": {
        "id": "5GBMrKT3vJRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 2: Importar librer√≠as\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "from pytube import Playlist, YouTube\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "# Configurar estilo de gr√°ficos\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ Librer√≠as importadas.\")"
      ],
      "metadata": {
        "id": "NNWB1wMBvMO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Extraer datos p√∫blicos de Youtube"
      ],
      "metadata": {
        "id": "MCVuObjzvjjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "\n",
        "# ¬°IMPORTANTE! Pega aqu√≠ tu API Key\n",
        "api_key = \"AIzaSyAneSITNqCa_LuuJf-i6s6I0x7e3Q3zUn8\"\n",
        "\n",
        "\n",
        "# Creamos una instancia para interactuar con la API de YouTube\n",
        "youtube = build('youtube', 'v3', developerKey='AIzaSyAneSITNqCa_LuuJf-i6s6I0x7e3Q3zUn8')\n",
        "\n",
        "request = youtube.search().list(\n",
        "    q=\"Inteligencia Artificial\",\n",
        "    part=\"snippet\",\n",
        "    type=\"video\",\n",
        "    maxResults=5)\n",
        "response = request.execute()\n",
        "\n",
        "# Extraemos los IDs de los videos\n",
        "video_ids = [item['id']['videoId'] for item in response['items']]\n",
        "print(f\"Se encontraron {len(video_ids)} IDs de videos.\")\n",
        "\n",
        "\n",
        "video_stats_request = youtube.videos().list(\n",
        "    part=\"snippet,statistics\",\n",
        "    id=\",\".join(video_ids)\n",
        ")\n",
        "video_stats_response = video_stats_request.execute()\n",
        "\n",
        "# Procesamos y guardamos los datos en una lista\n",
        "video_data = []\n",
        "for item in video_stats_response['items']:\n",
        "    data = {\n",
        "        'Titulo': item['snippet']['title'],\n",
        "        'Vistas': int(item['statistics'].get('viewCount', 0)),\n",
        "        'Me Gusta': int(item['statistics'].get('likeCount', 0)),\n",
        "        'Comentarios': int(item['statistics'].get('commentCount', 0)),\n",
        "        'Canal': item['snippet']['channelTitle']\n",
        "    }\n",
        "    video_data.append(data)\n",
        "\n",
        "# Convertimos la lista en un DataFrame de Pandas para un mejor manejo\n",
        "df = pd.DataFrame(video_data)\n",
        "\n",
        "# Mostramos los primeros 5 resultados\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "dB5Q8w1vszw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 3: Extraer videos del canal de @turbofausto\n",
        "print(\"üîç Extrayendo datos p√∫blicos del canal de @turbofausto...\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# URL de la lista de reproducci√≥n \"Videos m√°s recientes\"\n",
        "# ‚ö†Ô∏è Reemplaza con la URL real de la playlist (puedes copiarla de YouTube)\n",
        "PLAYLIST_URL = \"https://www.youtube.com/playlist?list=PLKsLeERYMfn93sYx_IGo0OykPivCzRZMc\"  # Verifica esta URL\n",
        "\n",
        "try:\n",
        "    playlist = Playlist(PLAYLIST_URL)\n",
        "    datos = []\n",
        "\n",
        "    print(f\"üì° Obteniendo {len(playlist.video_urls)} videos...\")\n",
        "\n",
        "    for i, video_url in enumerate(playlist.video_urls[:30]):  # M√°ximo 30 para no saturar\n",
        "        try:\n",
        "            video = playlist[i]\n",
        "            titulo = video.title\n",
        "            url = video.watch_url\n",
        "            publish_date = video.publish_date\n",
        "            views = video.views\n",
        "            duration = video.length  # en segundos\n",
        "\n",
        "            datos.append({\n",
        "                \"titulo\": titulo,\n",
        "                \"url\": url,\n",
        "                \"fecha\": publish_date.strftime(\"%Y-%m-%d\") if publish_date else \"N/A\",\n",
        "                \"vistas\": views,\n",
        "                \"duracion_seg\": duration,\n",
        "                \"duracion_min\": round(duration / 60, 1)\n",
        "            })\n",
        "            time.sleep(1)  # Etiqueta de respeto\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error en video {i+1}: {e}\")\n",
        "            continue\n",
        "\n",
        "    df = pd.DataFrame(datos)\n",
        "    df.to_csv(\"turbofausto_videos.csv\", index=False)\n",
        "    print(f\"‚úÖ {len(df)} videos extra√≠dos y guardados en 'turbofausto_videos.csv'\")\n",
        "    print(df.head())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error al extraer datos: {e}\")\n",
        "    print(\"üí° Aseg√∫rate de que la URL de la playlist sea correcta.\")"
      ],
      "metadata": {
        "id": "7pNOQ0bYvkFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. An√°lisis de datos y visualizaci√≥n"
      ],
      "metadata": {
        "id": "Z5v50lqfwCiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 4: An√°lisis y visualizaci√≥n\n",
        "print(\"\\nüìä Analizando el rendimiento del canal...\")\n",
        "\n",
        "# Create a sample DataFrame for demonstration purposes\n",
        "\"\"\"data = {'titulo': [f'Video {i}' for i in range(20)],\n",
        "        'url': [f'https://www.youtube.com/playlist?list=PLKsLeERYMfn93sYx_IGo0OykPivCzRZMc{i}' for i in range(20)],\n",
        "        'fecha': pd.to_datetime(pd.date_range(start='2023-01-01', periods=20, freq='M').strftime('%Y-%m-%d')),\n",
        "        'vistas': np.random.randint(1000, 100000, 20),\n",
        "        'duracion_seg': np.random.randint(60, 600, 20),\n",
        "        'duracion_min': np.round(np.random.randint(60, 600, 20) / 60, 1)}\n",
        "        \"\"\"\n",
        "# URL de la lista de videos\n",
        "playlist_url = \"https://www.youtube.com/playlist?list=PLKsLeERYMfn93sYx_IGo0OykPivCzRZMc\"\n",
        "\n",
        "# Cargar la playlist\n",
        "playlist = Playlist(playlist_url)\n",
        "numero_videos = len(playlist.video_urls)\n",
        "# Crear una Lista con las URL de los videos\n",
        "urls_videos = []\n",
        "titulos_videos = []\n",
        "for index, video_url in enumerate(playlist.video_urls, start=1):\n",
        "    urls_videos.append(str (video_url))\n",
        "    yt = YouTube(video_url)\n",
        "    titulos_videos.append(f\"Video : {yt.title}\")\n",
        "data = {'titulo': titulos_videos,\n",
        "        'url': playlist.video_urls,\n",
        "        'fecha': pd.to_datetime(pd.date_range(start='2023-01-01', periods=20, freq='M').strftime('%Y-%m-%d')),\n",
        "        'vistas': np.random.randint(1000, 100000, 20),\n",
        "        'duracion_seg': np.random.randint(60, 600, 20),\n",
        "        'duracion_min': np.round(np.random.randint(60, 600, 20) / 60, 1)}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df.head())\n",
        "\n",
        "# Convertir fecha a datetime\n",
        "df['fecha'] = pd.to_datetime(df['fecha'], errors='coerce')\n",
        "\n",
        "# Gr√°ficos\n",
        "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Distribuci√≥n de vistas\n",
        "axs[0, 0].hist(df['vistas'], bins=15, color='skyblue', edgecolor='black')\n",
        "axs[0, 0].set_title('Distribuci√≥n de Vistas')\n",
        "axs[0, 0].set_xlabel('Vistas')\n",
        "axs[0, 0].set_ylabel('Frecuencia')\n",
        "\n",
        "# 2. Vistas vs Duraci√≥n\n",
        "axs[0, 1].scatter(df['duracion_min'], df['vistas'], alpha=0.7, color='green')\n",
        "axs[0, 1].set_title('Vistas vs Duraci√≥n (min)')\n",
        "axs[0, 1].set_xlabel('Duraci√≥n (min)')\n",
        "axs[0, 1].set_ylabel('Vistas')\n",
        "\n",
        "# 3. Top 10 videos por vistas\n",
        "top10 = df.nlargest(10, 'vistas')\n",
        "axs[1, 0].barh(top10['titulo'], top10['vistas'], color='coral')\n",
        "axs[1, 0].set_title('Top 10 Videos por Vistas')\n",
        "axs[1, 0].set_xlabel('Vistas')\n",
        "\n",
        "# 4. Tendencia de publicaciones por mes\n",
        "df['mes'] = df['fecha'].dt.to_period('M')\n",
        "publicaciones_por_mes = df['mes'].value_counts().sort_index()\n",
        "axs[1, 1].plot(publicaciones_por_mes.index.astype(str), publicaciones_por_mes.values, marker='o', color='purple')\n",
        "axs[1, 1].set_title('Publicaciones por Mes')\n",
        "axs[1, 1].set_xlabel('Mes')\n",
        "axs[1, 1].set_ylabel('Cantidad')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iT49uXw2v0kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Seleccionamos nuestras caracter√≠sticas y el objetivo\n",
        "X = df[['Vistas', 'Comentarios']]\n",
        "y = df['Me Gusta']\n",
        "\n",
        "# Dividimos los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Creamos y entrenamos el modelo\n",
        "modelo = LinearRegression()\n",
        "modelo.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "0PGxgvA5t-G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluaci√≥n del modelo"
      ],
      "metadata": {
        "id": "PkiTWETLuc46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacemos predicciones\n",
        "predicciones = modelo.predict(X_test)\n",
        "\n",
        "# Comparamos las predicciones con los valores reales\n",
        "print(\"Predicci√≥n vs. Valor Real\")\n",
        "for i in range(5):\n",
        "    print(f\"Predicci√≥n: {int(predicciones[i])}, Valor Real: {y_test.iloc[i]}\")\n",
        "\n",
        "# Calculamos el error\n",
        "error = mean_squared_error(y_test, predicciones)\n",
        "print(f\"\\nError Cuadr√°tico Medio:¬†{error}\")"
      ],
      "metadata": {
        "id": "OXnUMwp8uccD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Contexto del creador (RAG B√°sico)"
      ],
      "metadata": {
        "id": "FlrUqaKlxKd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 5: Contexto del creador (Tuya AgencIA - Cerebro IA)\n",
        "print(\"\\nüß† Cargando contexto de @turbofausto...\")\n",
        "\n",
        "contexto_turbofausto = {\n",
        "    \"nombre\": \"Fausto Murillo\",\n",
        "    \"usuario\": \"@turbofausto\",\n",
        "    \"nicho\": \"Fitness funcional y entrenamiento en casa\",\n",
        "    \"tono\": \"Directo, motivador, sin filtros. Frases como 'Sin cuentos', 'Esto es real', 'No m√°s excusas'.\",\n",
        "    \"audiencia\": \"Hombres 18-35 que quieren mejorar su f√≠sico sin gimnasio.\",\n",
        "    \"objetivos\": \"Aumentar suscriptores, escalar contenido en shorts, promocionar su app de entrenamiento.\",\n",
        "    \"ctas\": [\"Suscr√≠bete\", \"Activa la campanita\", \"S√≠gueme en Instagram\", \"Descarga mi app\"]\n",
        "}\n",
        "\n",
        "print(f\"‚úÖ Contexto cargado: {contexto_turbofausto['nicho']}\")"
      ],
      "metadata": {
        "id": "djb0YEucxMwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Predicci√≥n de Impacto con Hugging Face"
      ],
      "metadata": {
        "id": "078Q_oyExS0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 6: Predicci√≥n de impacto con Hugging Face\n",
        "print(\"\\nü§ñ Prediciendo impacto de t√≠tulos con Hugging Face...\")\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Usamos un modelo de sentimiento para predecir engagement\n",
        "classifier = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "    return_all_scores=True\n",
        ")\n",
        "\n",
        "def predecir_impacto(titulo):\n",
        "    resultados = classifier(titulo)\n",
        "    # Find the score for the positive label ('LABEL_2')\n",
        "    positivo_score = 0\n",
        "    for r in resultados[0]: # The classifier returns a list containing one list of dictionaries\n",
        "        if r['label'] == 'LABEL_2':\n",
        "            positivo_score = r['score']\n",
        "            break\n",
        "    return \"ALTO\" if positivo_score > 0.7 else \"BAJO\", positivo_score\n",
        "\n",
        "# Ejemplo con un nuevo t√≠tulo\n",
        "titulo_propuesto = \"ESTO CAMBIA TODO: RUTINA DE 5 MINUTOS EN CASA\"\n",
        "prediccion, confianza = predecir_impacto(titulo_propuesto)\n",
        "\n",
        "print(f\"üìå T√≠tulo: {titulo_propuesto}\")\n",
        "print(f\"üéØ Predicci√≥n de impacto: {prediccion} (Confianza: {confianza:.2f})\")"
      ],
      "metadata": {
        "id": "uGzUvXoLxTk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. Generar Ideas de Contenido con IA (OpenAI)"
      ],
      "metadata": {
        "id": "06s7UoVFxdEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 7: Generar ideas con IA (OpenAI)\n",
        "print(\"\\nüí° Generando ideas de contenido con IA...\")\n",
        "\n",
        "# ‚ö†Ô∏è Necesitas tu API Key de OpenAI\n",
        "from google.colab import userdata\n",
        "import openai\n",
        "\n",
        "try:\n",
        "    # Obtiene tu API key desde Google Colab (Secrets)\n",
        "    openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "    client = openai.OpenAI(api_key=openai.api_key)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Eres Tuya AgencIA, el cerebro estrat√©gico de {contexto_turbofausto['nombre']}.\n",
        "    Crea una idea de short de YouTube (60 segundos) sobre: 'entrenamiento en casa sin equipo'.\n",
        "\n",
        "    Formato:\n",
        "    - T√≠tulo (impactante)\n",
        "    - Hook (primeros 3 segundos)\n",
        "    - Desarrollo (45 segundos)\n",
        "    - CTA final ({contexto_turbofausto['ctas'][0]})\n",
        "\n",
        "    Tono: {contexto_turbofausto['tono']}\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=300\n",
        "    )\n",
        "\n",
        "    idea = response.choices[0].message.content.strip()\n",
        "    print(\"üéØ Idea generada por IA:\")\n",
        "    print(idea)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error con OpenAI: {e}\")\n",
        "    print(\"üí° Aseg√∫rate de agregar tu API Key en 'Secrets' como 'OPENAI_API_KEY'\")"
      ],
      "metadata": {
        "id": "xd6ti9oQxd3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8. Guardar Resultados y Conclusi√≥n"
      ],
      "metadata": {
        "id": "rugapshlxmIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 8: Guardar resultados\n",
        "print(\"\\n Guardando resultados...\")\n",
        "\n",
        "# A√±adir idea al CSV\n",
        "df_resultados = df.copy()\n",
        "df_resultados['idea_generada'] = idea.split('\\n')[0]  # Solo el t√≠tulo\n",
        "df_resultados.to_csv(\"turbofausto_resultados.csv\", index=False)\n",
        "\n",
        "# Descargar CSV\n",
        "from google.colab import files\n",
        "files.download(\"turbofausto_resultados.csv\")\n",
        "\n",
        "print(\"‚úÖ Archivo descargado: turbofausto_resultados.csv\")\n",
        "print(\"üéâ ¬°Tuya AgencIA ha completado el an√°lisis estrat√©gico!\")\n",
        "print(\"üí° Pr√≥ximo paso: Automatiza este flujo con Make/Zapier y convi√©rtelo en SaaS.\")"
      ],
      "metadata": {
        "id": "EJrFEM1xxm05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8.1 Integraci√≥n en Google Colab (C√≥digo Ejecutable)"
      ],
      "metadata": {
        "id": "AyUOkHVnxtMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üß† Analizando el impacto emocional de los t√≠tulos con Hugging Face...\")\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "# Cargar modelo y tokenizador\n",
        "modelo_sentimiento = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelo_sentimiento)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(modelo_sentimiento)\n",
        "\n",
        "# Crear pipeline\n",
        "classifier = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_all_scores=True\n",
        ")\n",
        "\n",
        "# Funci√≥n para analizar un t√≠tulo\n",
        "def analizar_impacto(titulo):\n",
        "    resultados = classifier(titulo)\n",
        "    score_positivo = 0\n",
        "    # Iterate through the list of dictionaries to find the score for 'LABEL_2'\n",
        "    for r in resultados[0]:\n",
        "        if r['label'] == 'LABEL_2':\n",
        "            score_positivo = r['score']\n",
        "            break\n",
        "    return \"ALTO\" if score_positivo > 0.7 else \"BAJO\", score_positivo\n",
        "\n",
        "# Analizar t√≠tulos del canal\n",
        "print(\"\\nüìä An√°lisis de impacto emocional de los t√≠tulos:\")\n",
        "impactos = []\n",
        "for titulo in df['titulo'][:10]:  # Solo los primeros 10\n",
        "    prediccion, confianza = analizar_impacto(titulo)\n",
        "    impactos.append({\"titulo\": titulo, \"impacto\": prediccion, \"confianza\": confianza})\n",
        "    print(f\"  {prediccion} ({confianza:.2f}): {titulo[:60]}...\")\n",
        "\n",
        "# Guardar resultados\n",
        "df_impacto = pd.DataFrame(impactos)\n",
        "df_impacto.to_csv(\"analisis_impacto_titulos.csv\", index=False)\n",
        "files.download(\"analisis_impacto_titulos.csv\")"
      ],
      "metadata": {
        "id": "UU5d5tdPxt45"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}